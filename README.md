# US Wildfires Analysis: Data Optimization & Memory Management ðŸ“‰ðŸ”¥

## ðŸ‡ºðŸ‡¸ English Version
###  Project Overview
This project focuses on exploring the **1.88 Million US Wildfires** dataset. The primary goal is to demonstrate advanced data manipulation techniques using **Pandas**, specifically focusing on **Memory Management** and **Data Efficiency** when working with large-scale datasets on resource-constrained hardware (standard laptops).

###  Optimization Techniques
* **Feature Selection:** Loading only essential columns using `usecols`.
* **Data Downcasting:** Converting data types to save up to 70% of RAM.
* **Chunk Processing:** Processing the dataset in smaller batches to prevent memory overflow.

---

## ðŸ‡®ðŸ‡© Versi Bahasa Indonesia
<details>
  <summary><b>Klik untuk melihat detail (Bahasa Indonesia)</b></summary>

  ###  Ikhtisar Proyek
  Proyek ini berfokus pada eksplorasi dataset **1,88 Juta Kebakaran Hutan di AS**. Tujuan utamanya adalah mendemonstrasikan teknik manipulasi data tingkat lanjut menggunakan **Pandas**, khususnya pada **Manajemen Memori** dan **Efisiensi Data** saat bekerja dengan dataset skala besar di perangkat dengan spesifikasi terbatas.

  ###  Teknik Optimasi yang Digunakan
  * **Pemilihan Fitur (Feature Selection):** Memuat kolom penting saja menggunakan `usecols`.
  * **Downcasting Data:** Mengubah tipe data untuk menghemat penggunaan RAM hingga 70%.
  * **Pemrosesan Bertahap (Chunking):** Membaca dataset dalam potongan kecil untuk menghindari kelebihan beban memori.
</details>

---
*Developed as part of my Data Science journey. "Large data, smart solutions."*